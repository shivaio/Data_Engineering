when  this  deep  revival  happened  so  in  two thousand and six  a  very  important  study  was  or  a  very 
important contribution was made by hinton and salakhutdinov 
sorry if i have not pronounced it properly and they found that a method for training very 
deep neural network effectively now again the details of these are not important we 
will be doing that in the course at some point but what is the important take away here is 
that while from one thousand, nine hundred and eighty-nine to two thousand and six we knew that there is an algorithm for training deep neural 
networks and they can potentially be used for solving a wide range of problems because 
that is what the universal approximation theorem said  but the problem was that in 
practice we were not being able to use it for much 
it was not easy to train these networks but now with this technique there was revived 
interest and hope that now actually  can train  very deep neural networks for lot of 
practical problems this sparked off the interest again and then people started looking at 
all such of thing right that even this particular study which was done in two thousand and six  will 
actually be very simple to something done way back in nine thousand, one hundred and ninety-three and which again showed 
that you can train a very deep neural network but again due to several factors may be at 
that time due to the computational requirements or the data requirements or whatever i 
am not too sure about that it did not become so popular then but by two thousand and six probably the 
stage was much better for these kind of networks or techniques to succeed so then it 
became popular in two thousand and six 
then   this   two thousand and six   to   two thousand and nine   people   started   gaining   more   and   more   insights   into   the 
effectiveness of this discovery made by  hinton and others which is unsupervised pre 
training  right  that is  what  i spoke  about  on the previous  slide unsupervised  pre 
training 
and they started getting more and more insights into how you can make deep neural 
networks really work so they came up with various techniques some of which we are 
going to study in this course so this was about how do you initialise the network better 
what   is   the   better   optimization   algorithm   to   use   what   is   the   better   regularization 
algorithm to use and so on so there were many things which were started coming out at 
this period two thousand and six to two thousand and nine and by two thousand and nine everyone started taking note of this and again deep 
neural networks of artificial neural networks started becoming popular 
that is when people realised that all this all the negative things that were tied to it that 
you are not able to train it well and so on have slowly  people have started finding 
solutions to get by those and maybe we should start again focusing on the potential of 
deep neural networks and see if they can be used for large scale practical application so 
this two thousand and six to two thousand and nine was again a slow boom period were people were again trying to do a 
lot of work to popularize  deep neural  networks and get rid of some of the problems 
which existed in training them 
now from two thousand and nine onwards there was this series of success is which kind of caught 
everyone which made everyone to stand up and take notice  right  that this is really 
working for a lot of practical applications starting with handwriting recognition  so 
around two thousand and nine these guys won handwriting recognition competition in arabic and they did 
way better than the competitor systems using a deep neural network and then this was a 
success 
so this was an handwriting recognition and then there was speech so this shown that 
various   existing   systems   the   error   rate   of   these   system   could   be   seriously  be 
significantly   reduced   by   using   deep   neural   networks   or   plugging   in   a   deep   neural 
network component to existing systems right so this was handwriting and then speech 
then again some kind of pattern recognition which was on handwritten digit recognition 
for mnist this is a very popular data set which had been around since ninety-eight and a new 
record was set on this data so this is the highest accuracy that was achieved on this data 
set around that time in two thousand and ten sorry and this is also the time when gpus entered the same 
so before that all of the stuff was being done on cpus but then people realised that 
very deep neural networks require a lot of computation and lot of this computation can 
happen very quickly on gpus as opposed to cpus 
so people started using gpus for training and that drastically reduced the training and 
inference time  so  that was again  something  which sparked a lot of interest  right 
because even though these were successful they were taking a lot of time to train but 
now the gpus could even take care of that and this success continued 
so  people   started   gaining   or   getting   success   in   other   fields   like   visual   pattern 
recognition so this was a competition on recognising traffic sign boards and here again 
a deep neural network did way better than its other competitors 
and then the most popular or one thing which made neural networks really popular was 
this image net challenge which was around since two thousand and eight or two thousand and nine and before two thousand and twelve when 
this  alexnet was one of the participating  systems in  this competition  most of the 
systems were non neural network based systems and this competition was basically 
about classifying a given image into one of thousand classes 
so this could be an image of a bird or a dog or a human or car truck and so on say you 
have to identify the right class of the main object in the image so in two thousand and twelve this alexnet 
which was a deep neural network or a convolutional neural network based system was 
able to actually outperform all the other systems by a margin of sixty-seven percent so the error 
for this system was sixteen percent and this is a deep neural network because it had eight layers 
the next year this was improved further and something known as zf network propose 
which was again eight layers but it did better than alexnet the next year even a deeper 
network with nineteen layers was proposed which did significantly better than alexnet then 
google entered the scene and they proposed something which is twenty-two layers and again 
reduced the error then microsoft joined in and they proposed something which had one hundred and fifty-two 
layers and the error that you see here is actually better than what humans do 
so even if a human was asked to label this image because of certain law certain noise in 
the image and so on even a human is bound to make more errors than thirty-six per cent that 
means even if you show hundred images to humans he or she is bound to may go wrong 
or more than three or four of these images right there is this system was able to get an 
error of thirty-six per cent over the large test set 
so this two thousand and twelve to two thousand and sixteen period were there was this continuous success on the image net 
challenge   as   well   as   successes   in   other   fields   like   natural   language   processing 
handwriting recognition speech and so on so this is the period where now everyone 
started talking about deep learning and lot of company started investing in it a lot of 
traditional systems which were not deep neural network based was now started people 
started converting them to deep neural network based system 
so translation system speed systems image classification object detection and so on 
there   were  lot   of   success   in   all   these   fields   using   deep   neural   networks  and   this 
particular thing that we are talking about which is image net and the success in this was 
driven by something known as convolutional neural networks 
