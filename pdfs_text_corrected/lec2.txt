we will start talking about artificial intelligence and this is titled as from the spring to 
the winter of ai so i am going to talk about when was this boom in ai started or 
when is that people started thinking and talking about ai seriously and what eventually 
happened to the initial boom and so 
so  let   us  start   with  one thousand, nine hundred and forty-three  whereas   i   saying   that   there   was   a   lot   of   interest   in 
understanding how does a human brain work and then come up with a computational 
or   a   mathematical   model   of   that  so  mcculloch   and   pitts   one   of   them   was   a 
neuroscientist and the other one was a logician no computer scientists or anything at that 
point of time 
and they came up with this extremely simplified model that just as a brain takes a input 
from lot of factors so now suppose you want to decide whether you want to go out for a 
movie or not so you would probably think about do you really have any exams coming 
up that could be our factor x1 you could think about is a weather good to go out is it 
raining would it be difficult to go out at this point would there be a lot of traffic is it a 
very popular movie and hence tickets may not be available and so on 
so being kind of presses all this information you might also look at things like the 
reviews of the movie or the imdb rating of the movie and so on and based on all these 
complex inputs it applies some function and then takes a decision yes or no that i want 
to probably go for a movie 
so this is an overly simplified model of how the brain works is and what this model 
says is that you take inputs from various sources and based on that you come up with the 
binary decision right so this is what they proposed in one thousand, nine hundred and forty-three so now we have come to 
an artificial neuron so this is not a biological neuron this is how you would implement 
it as a machine right so that was in one thousand, nine hundred and forty-three 
then along and then this kind of led  to a lot of boom in our interest in artificial 
intelligence and so on and i guess around one thousand, nine hundred and fifty-six in a conference the term artificial 
intelligence   was   a   formally   coined   and   within   a   one   or   two   years   from   there   frank 
rosenberg   came   up   with   this   perceptron   model   of   doing   computations   or   what 
perceptron model of what an artificial neuron could be 
and we will talk about this in detail later on the course and not tell you what these things 
are as of now just think of the a new model was proposed and this is what he had to say 
about this model right so he said that the perceptron may eventually be able to learn 
make decisions and translate languages do you find anything odd about this statement 
yeah so learn and make decisions make sense but why translate languages why is so 
specific why such a specific interest in languages 
so that you have to connect back to history so this is also the period of the cold war 
and there was always always a lot of interest there was lot of research and translation 
was actually fuelled by the world war and evens that happened after that where these 
countries which were at loggerheads with each other 
they wanted to understand what the other country is doing but they did not speak each 
otherâ€™s language that is why there was a lot of interest from espionage point of view or 
from spying and so on to be able to translate languages and hence that specific require 
and lot of this research would have been funded from agencies which are interested in 
these things right and the defence or war or something 
so and this work was largely done for the navy and this is an this is an extract from the 
article written in new york times way back in one thousand, nine hundred and fifty-seven or fifty-eight where it was mentioned that 
the embryo often this perceptron is an embryo of an electronic computer that the navy 
expects will be able to walk talk see write reproduce itself and be conscious of it is 
existence 
so i am not quoting something from two thousand and seventeen or eighteen this is way back in one thousand, nine hundred and fifty-seven fifty-eight why i am 
that is why i like the history part of it so recently there is a lot of boom or a lot of hype 
around ai that ai will take over a lot of things will take our jobs it might eventually we 
might be colonized by ai agents and so on 
so i just want to emphasize that i do not know whether that will happen or not but this 
is not something new we have been talking about the promise of ai as far back since 
one thousand, nine hundred and fifty-seven one thousand, nine hundred and fifty-eight right this not something new that people are talking about now it is always 
been there and to what extent this promise will be fulfilled is yet to be seen 
and of course as compared to one hundred and ninety-five thousand, seven hundred and fifty-eight we have made a lot of progress in other fields 
which have enabled ai to be much more successful than it was earlier for example we 
have much better compute power now we have lots of data now and all thanks to the 
internet and other things that you can actually crawl tons and tons of data and then try to 
learn something from a data or try to make the machine learn something from it 
so we have made a lot of progress in other aspects where which ai is now at a position 
where it can really make a difference but just wanted to say that these are not things 
which i have not been said in the past it has always been the it has always been 
considered to be very promising and perhaps a bit hyped also so that is about one hundred and ninety-five thousand, seven hundred and fifty-eight 
then now what we talk about what is all the for the past eight to ten years at least when 
we talk about ai talking about deep learning and that is what this course  is about 
largely about deep learning i am not saying that other and what deep learning is largely 
about if i want to tell you in a very layman nutshell term is it is about a large number 
of artificial neurons connected to each other in layers and functioning towards achieving 
certain goal 
so this is like a schematic of what a deep neural network or a feed forward neural 
network would look like now this is again not something new which is up in the last eight 
to ten years although people have started discussing it a lot in the last eight to ten years 
look at it way back in  one hundred and ninety-six thousand, five hundred and sixty-eight opposed something which looked very much like a 
modern deep neural network or a modern feed forward neural network 
and in many circles he is considered to be one of the founding fathers of modern deep 
learning 
so that is about that  right from one thousand, nine hundred and forty-three to one thousand, nine hundred and sixty-eight it was mainly 
about the springtime for ai and what i mean by that that everyone was showing 
interest in that the government was funding a lot of research in ai and people really 
thought that ai could deliver a lot of things on a lot of fronts  for 
various applications health care defence and so on 
and then around one thousand, nine hundred and sixty-nine an interesting paper came out by these two gentlemen minsky 
and papert which essentially outlined some limitations of the perceptron model and we 
will talk about these limitations later on in the course in the second or third lecture but 
for now i will not get into a details of that but what it is said that it is possible that a 
perceptron cannot handle some very simple functions also 
so you are trying to make the perceptron learn some very complex functions because 
the way we decide how to watch a movie is a very complex function of the inputs that 
we considered but even a simple function like xor or is something which a perceptron 
cannot be used to model that is what this paper essentially showed and this led to severe 
criticism for ai and then people started losing interest in ai and lot of government 
funding actually subsided after one thousand, nine hundred and sixty-nine 
all the way to one thousand, nine hundred and eighty-six actually this was the ai winter of connectionism so there was very 
little interest in connectionist ai so there are two types of ai one is symbolic ai and the 
other is connectionist ai so whatever we are going to study in this course about neural 
networks and all that probably falls in connectionist ai paradigm and there was no 
interest in this and people i mean hard to get funding and so on for these seventeen to eighteen years 
and that was largely triggered by this study that was done by minsky and papert and 
interestingly they were also often misquoted and what they had actually said in that 
papers so they had said a single perceptron cannot do it they in fact said that a multi 
layer network of perceptrons can do it  but  no one focused on the second part that a 
multilayer network of perceptron people started pushing the idea that a perceptron 
cannot do it and hence we should not be investigating it and so on right so that is 
what happened for a long time and this known as the winter the first winter 
then around one thousand, nine hundred and eighty-six actually came this algorithm which is known as back propagation 
again this is an algorithm which we are going to cover in a lot of detail in the course in 
the 4th or 5th lecture and this algorithm actually enables to train a deep neural network 
right so deep network of neurons is something that you can train using this algorithm 
now this algorithm was actually popularized by at rumelhart and others in one thousand, nine hundred and eighty-six but it 
is not completely discovered by them this was also around in various other fields so it 
was there  in  i think in systems analysis or something like that it was being used for 
other purposes in a different context and so on and rumelhart other and others in one thousand, nine hundred and eighty-six 
were the first to kind of popularize it in the context of deep neural networks 
and this was a very important discovery because even today all the neural network so 
most of them are trained using back propagation right and of course there have been 
several other advances but the core remains the same that you use back propagation to 
train a deep neural network right so something this was discovered almost thirty years 
back is still primarily used for training deep neural networks that is why this was a very 
important paper or breakthrough at that time 
 
and  around   the  same  time  so  again   interestingly  so  back   propagation   is  used   in 
conjunction with something known as gradient descent which was again discovered 
way back in one thousand, eight hundred and forty-seven by cauchy and he was interested in using this to compute the orbit of 
heavenly bodies 
that is something that people care about at that time today of course we use it for 
various  other  purposes one  of them  being  discovering  cats  and  videos  or even  for 
medical imaging or for describing whether certain have of cancer is being depicted in a 
xray or things like that there is a lot of other purposes for which deep neural networks 
enhance and hence back propagation gradient descent and other things are being used 
for it but again these are not very modern discoveries these are dated way back thirty years 
and even gradient descent is almost one hundred and fifty years and so on so that is what i wanted to 
emphasize 
and around the same time in one thousand, nine hundred and ninety or one thousand, nine hundred and eighty-nine there is this another interesting theorem 
which was proved which is known as the universal approximation theorem and this is 
again something that we will cover in the course in the third lecture or something like 
where we will talk about the power of a deep neural network 
so again the importance of this or why this theorem was important will become clear 
later and when we cover it in detail but for now it is important to understand that what 
this theorem said is that if you have a deep neural network you could basically model all 
types of functions continuous functions to any desired precision 
so what it means in very layman terms is that if the way you make decisions using a 
bunch of inputs is a very very complex function of the input then you can have a neural 
network which will be able to learn this function right in many laymen terms that is 
what it means 
and if i have to hype it up a bit or i have to say it in a very enthused and excited manner 
i would say that basically it says that deed neural networks can be used for solving all 
kinds of machine learning problems and that is roughly what it says but with a pinch of 
salt and a lot of caveats but that is what it means at least in the context of this course 
so   this   is   all   around   one thousand, nine hundred and eighty-nine   and   despite   this   happening   some   important   discoveries 
towards  the  late  end  of  80â€™s  which  was  back   propagation   universal  approximation 
theorem people were still not being able to use deep neural networks for really solving 
large practical problems and a few challenges there was of course the compute power at 
that time was not at a level where it could support deep neural networks 
we do not have enough data for training deep neural networks and also in terms of 
techniques while back propagation is a sound technique it is to fail when you have 
really deep neural network so when people try it training a very deep neural network 
they found that the training does not really converge the system does not really learn 
anything and so on and there were certain issues with using back propagation off the 
shelf at that time because of which it was not very successful 
so again despite these slight boom around eighty-six to ninety where some important discoveries 
were made and even follow up in ninety-two ninety-three and so on there is still not a real big hype 
around deep neural networks or artificial neural networks and at time again a slump a 
slow winter right up till two thousand and six 
